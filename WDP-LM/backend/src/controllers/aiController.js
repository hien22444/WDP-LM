const OpenAI = require("openai");
const TutorProfile = require("../models/TutorProfile");

// Prefer several env var names (developer may set CHATAI_KEY or CHATAI)
const preferredKeyNames = [
  "CHATAI_KEY",
  "CHATAI",
  "CHAT_AI_KEY",
  "OPENAI_API_KEY",
  "OPENAI_KEY",
];
let OPENAI_KEY = "";
let OPENAI_KEY_NAME = null;
for (const n of preferredKeyNames) {
  if (process.env[n] && String(process.env[n]).trim()) {
    OPENAI_KEY = String(process.env[n]).trim();
    OPENAI_KEY_NAME = n;
    break;
  }
}

console.log("[DEBUG] OpenAI key var =", OPENAI_KEY_NAME || "(none)");
console.log("DEBUG: AI key length =", OPENAI_KEY?.length || 0);
let aiEnabled = true;
let openai = null;

// detect key type for logging and guidance
function detectOpenAIKeyType(key) {
  if (!key) return "none";
  if (key.startsWith("sk-proj")) return "project";
  if (key.startsWith("sk-svcacct")) return "service-account";
  if (key.startsWith("sk-")) return "secret";
  return "unknown";
}

const keyType = detectOpenAIKeyType(OPENAI_KEY);

if (!OPENAI_KEY) {
  // No OpenAI key found locally. Check whether a Google/Gemini key is provided
  // via env (server.js supports GEMINI_KEY/GOOGLE_API_KEY). If so, enable AI
  // here and rely on the server-attached client (req.app.locals.openai) at
  // request time.
  const providerEnv = (
    process.env.CHATAI_PROVIDER ||
    process.env.CHATAI_PROVIDER_NAME ||
    ""
  ).toLowerCase();
  const googleKey =
    process.env.GEMINI_KEY ||
    process.env.GEMINI_API_KEY ||
    process.env.GOOGLE_API_KEY ||
    process.env.GOOGLE_KEY ||
    process.env.CHATAI_KEY;
  if ((providerEnv === "google" || providerEnv === "gemini") && googleKey) {
    aiEnabled = true;
    openai = null; // will use app.locals.openai when handling a request
    console.log(
      `[AI] No local OpenAI key, but detected ${providerEnv} key in env; AI enabled (using server-provided client if attached).`
    );
  } else {
    console.error("[AI] OPENAI_API_KEY is missing. Using fallback only.");
    aiEnabled = false;
    console.warn("‚ö†Ô∏è AI mode disabled. Using fallback responses only.");
  }
} else {
  // If it's a project/service key, we normally recommend using a secret key.
  if (keyType === "project" || keyType === "service-account") {
    console.warn(
      `[AI] Detected a non-secret OpenAI key type: ${keyType}. By default this is not recommended.`
    );
    // If developer explicitly allows non-secret keys via env, try to initialize client anyway
    const allowNonSecret =
      String(process.env.ALLOW_NONSECRET_OPENAI_KEYS || "").toLowerCase() ===
      "true";
    if (!allowNonSecret) {
      console.error(
        `[AI] To enable using this key type set ALLOW_NONSECRET_OPENAI_KEYS=true in backend/.env. AI will remain disabled.`
      );
      aiEnabled = false;
    }
  }

  if (aiEnabled) {
    try {
      // prefer app-level client if provided by server.js
      if (global?.app && global.app.locals?.openai) {
        openai = global.app.locals.openai;
        console.log(
          "[AI] Using app-level OpenAI client from global.app.locals"
        );
      } else if (typeof require !== "undefined") {
        // fallback to local init
        openai = new OpenAI({ apiKey: OPENAI_KEY });
        console.log(`[AI] OpenAI client initialized (keyType=${keyType}).`);
      }
      if (keyType !== "secret") {
        console.warn(
          "‚ö†Ô∏è You are using a non-secret OpenAI key. Calls may fail if the key lacks permissions or billing. Check OpenAI Dashboard if you see errors."
        );
      }
    } catch (e) {
      console.error(
        "[AI] Failed to initialize OpenAI client:",
        e?.message || e
      );
      aiEnabled = false;
    }
  }

  // If server.js attached an app-level AI client (e.g., Gemini/Google wrapper),
  // enable AI even when this module didn't find a local OPENAI_KEY.
  try {
    if (!aiEnabled && global?.app && global.app.locals?.openai) {
      openai = global.app.locals.openai;
      aiEnabled = true;
      console.log(
        "[AI] Detected app-level AI client; enabling AI and using that client"
      );
    }
  } catch (e) {
    // ignore
  }
}

// Extra safety: if OPENAI_KEY was missing earlier we still want to detect
// an app-level AI client attached by server.js. This runs regardless of
// the earlier branch to ensure aiEnabled is correctly set when using
// Gemini/Google wrapper.
try {
  if (!aiEnabled && global?.app && global.app.locals?.openai) {
    openai = global.app.locals.openai;
    aiEnabled = true;
    console.log("[AI] App-level AI client detected (post-init); AI enabled");
  }
} catch (e) {
  // ignore
}

const conversationHistory = [];
const systemPrompt = `B·∫°n l√† chatbot th√¢n thi·ªán gi√∫p ng∆∞·ªùi d√πng t√¨m gia s∆∞. Khi ng∆∞·ªùi d√πng ch√†o, h√£y ch√†o l·∫°i t·ª± nhi√™n v√† th√¢n thi·ªán. Khi h·ªç h·ªèi t√¨m gia s∆∞, ph√¢n t√≠ch c√°c y·∫øu t·ªë: m√¥n h·ªçc, c·∫•p ƒë·ªô, h√¨nh th·ª©c (online/offline), ƒë·ªãa ƒëi·ªÉm v√† y√™u c·∫ßu kh√°c. ƒê·ªÅ xu·∫•t gia s∆∞ ph√π h·ª£p d·ª±a tr√™n chuy√™n m√¥n, kinh nghi·ªám, m·ª©c h·ªçc ph√≠ v√† l·ªãch d·∫°y. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, th√¢n m·∫≠t.`;

function resolveModelForClient(client, defaultModel = "gpt-3.5-turbo") {
  try {
    const prov = (
      process.env.CHATAI_PROVIDER ||
      process.env.CHATAI_PROVIDER_NAME ||
      ""
    ).toLowerCase();
    // If developer explicitly set GEMINI_MODEL / GOOGLE_MODEL use them
    if (prov === "gemini") {
      // Use text-bison-001 as a safer default for Gemini/Google generative API
      return process.env.GEMINI_MODEL || "text-bison-001";
    }
    if (prov === "google" || (client && client.provider === "google")) {
      return process.env.GOOGLE_MODEL || "text-bison-001";
    }
  } catch (e) {
    // ignore
  }
  return defaultModel;
}

function fallbackReply(userMsg) {
  const lower = (userMsg || "").toLowerCase();
  if (!userMsg || userMsg.trim() === "") return "B·∫°n ch∆∞a nh·∫≠p g√¨ c·∫£ üòä";
  if (lower.includes("xin ch√†o") || lower.includes("ch√†o"))
    return "Ch√†o b·∫°n! M√¨nh c√≥ th·ªÉ gi√∫p b·∫°n t√¨m gia s∆∞. B·∫°n c·∫ßn m√¥n g√¨ v√† ·ªü ƒë√¢u?";
  if (lower.includes("to√°n"))
    return "B·∫°n ƒëang t√¨m gia s∆∞ To√°n. B·∫°n mu·ªën h·ªçc ·ªü H√† N·ªôi hay TP.HCM?";
  if (lower.includes("anh"))
    return "B·∫°n ƒëang t√¨m gia s∆∞ Ti·∫øng Anh. B·∫°n c·∫ßn luy·ªán giao ti·∫øp hay ng·ªØ ph√°p?";
  return "M√¨nh ƒë√£ nh·∫≠n tin nh·∫Øn c·ªßa b·∫°n. Hi·ªán t·∫°i d·ªãch v·ª• AI ch∆∞a kh·∫£ d·ª•ng, b·∫°n c√≥ th·ªÉ cho bi·∫øt r√µ m√¥n h·ªçc, c·∫•p ƒë·ªô v√† ƒë·ªãa ƒëi·ªÉm kh√¥ng?";
}

function logOpenAIError(err) {
  console.error("‚ùå [AI] Error in AI tutor search:", err?.message || err);
  if (err?.response) {
    console.error("üî¥ [AI] Response status:", err.response.status);
    if (err.response.data)
      console.error(
        "üî¥ [AI] Response data:",
        JSON.stringify(err.response.data, null, 2)
      );
  } else if (err?.error) {
    console.error(
      "üî¥ [AI] OpenAI error detail:",
      JSON.stringify(err.error, null, 2)
    );
  } else {
    console.error("‚ö†Ô∏è [AI] Unknown error structure:", err);
  }
}

// Local reply generator: build a friendly Vietnamese summary from items
function localGenerateReply(userMsg, items) {
  if (!items || !items.length) return fallbackReply(userMsg);
  const top = items.slice(0, 5);
  let reply = `M√¨nh ƒë√£ t√¨m th·∫•y ${items.length} gia s∆∞ ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n.`;
  reply += "\n\n";
  reply += top
    .map((t, i) => {
      const subjects =
        (t.subjects || []).filter(Boolean).join(", ") || "Ch∆∞a c·∫≠p nh·∫≠t";
      const city = t.city ? ` ‚Äî ${t.city}` : "";
      const rating = t.avgRating ? ` ‚Äî ‚≠ê ${t.avgRating}/5` : "";
      const courses = (t.courses || []).length
        ? ` ‚Ä¢ ${t.courses.length} kh√≥a`
        : "";
      return `${i + 1}. ${t.name}${city} ‚Äî ${subjects}${rating}${courses}`;
    })
    .join("\n");
  reply +=
    "\n\nB·∫°n mu·ªën xem chi ti·∫øt gia s∆∞ n√†o (g√µ s·ªë th·ª© t·ª±), ho·∫∑c l·ªçc th√™m?";
  return reply;
}

// --- Helpers: sanitize tutors, parse query, fetch tutors ---
function sanitizeTutors(tutors) {
  // try to enrich with courses and review counts (best-effort)
  const TeachingSlot = (() => {
    try {
      return require("../models/TeachingSlot");
    } catch (e) {
      return null;
    }
  })();

  const TeachingSession = (() => {
    try {
      return require("../models/TeachingSession");
    } catch (e) {
      return null;
    }
  })();

  return Promise.all(
    (tutors || []).map(async (t) => {
      const base = {
        id: t._id,
        name: t.user?.full_name || "Gia s\u01b0",
        avatar: t.user?.avatar || t.avatarUrl || null,
        subjects: (t.subjects || []).map((s) => s.name),
        levels: (t.subjects || []).map((s) => s.level).filter(Boolean),
        experienceYears: t.experienceYears || 0,
        city: t.city || null,
        teachModes: t.teachModes || [],
        sessionRate: t.sessionRate || null,
        bio: t.bio || null,
        reviewCount: t.reviewCount || 0,
        avgRating: t.avgRating || null,
        recentReviews: [],
        courses: [],
      };

      if (TeachingSlot) {
        try {
          const courses = await TeachingSlot.find({
            tutorProfile: t._id,
            status: { $ne: "deleted" },
          })
            .limit(5)
            .lean();
          base.courses = (courses || []).map((c) => ({
            id: c._id,
            courseName: c.courseName,
            start: c.start,
            end: c.end,
            price: c.price,
            mode: c.mode,
          }));
        } catch (e) {
          // ignore
        }
      }

      // Enrich with reviews/ratings from TeachingSession (if available)
      if (TeachingSession) {
        try {
          const agg = await TeachingSession.aggregate([
            { $match: { tutorProfile: t._id, rating: { $ne: null } } },
            {
              $group: {
                _id: "$tutorProfile",
                avgRating: { $avg: "$rating" },
                totalReviews: { $sum: 1 },
              },
            },
          ]).exec();

          if (agg && agg[0]) {
            base.avgRating = Math.round((agg[0].avgRating || 0) * 10) / 10;
            base.reviewCount = agg[0].totalReviews || base.reviewCount || 0;
          }

          const recent = await TeachingSession.find({
            tutorProfile: t._id,
            feedback: { $ne: null },
          })
            .sort({ created_at: -1 })
            .limit(3)
            .select("rating feedback student startTime")
            .populate("student", "full_name")
            .lean();

          base.recentReviews = (recent || []).map((r) => ({
            rating: r.rating || null,
            feedback: r.feedback || null,
            studentName: r.student?.full_name || null,
            date: r.startTime || r.created_at || null,
          }));
        } catch (e) {
          // ignore enrichment errors
        }
      }

      return base;
    })
  );
}

function parseQueryToFilters(text) {
  const q = String(text || "").toLowerCase();
  const filters = {};
  const subjects = [];
  if (q.includes("to√°n") || q.includes("toan")) subjects.push("To√°n");
  if (q.includes("anh")) subjects.push("Ti·∫øng Anh");
  if (subjects.length) filters.subjects = subjects;
  if (q.includes("h√† n·ªôi") || q.includes("ha noi")) filters.city = "H√† N·ªôi";
  if (q.includes("h·ªì ch√≠ minh") || q.includes("tp.hcm") || q.includes("tphcm"))
    filters.city = "TP.HCM";
  if (q.includes("online")) filters.teachModes = "online";
  if (q.includes("offline")) filters.teachModes = "offline";
  return filters;
}

async function fetchRelevantTutorsFromDB(queryText, limit = 5) {
  try {
    const filters = parseQueryToFilters(queryText);
    const mongoQuery = { status: "approved" };
    if (filters.subjects)
      mongoQuery["subjects.name"] = { $in: filters.subjects };
    if (filters.city) mongoQuery.city = filters.city;
    if (filters.teachModes) mongoQuery.teachModes = filters.teachModes;

    const tutors = await TutorProfile.find(mongoQuery)
      .populate("user", "full_name avatar")
      .sort({ experienceYears: -1 })
      .limit(limit)
      .lean();

    return tutors;
  } catch (err) {
    console.error("[AI] fetchRelevantTutorsFromDB error:", err?.message || err);
    return [];
  }
}

// ===== Chat endpoint =====
const chatCompletion = async (req, res) => {
  try {
    const userMsg = req.body.message?.trim();
    if (!userMsg)
      return res.json({
        success: true,
        message: "B·∫°n ch∆∞a nh·∫≠p tin nh·∫Øn n√†o c·∫£ üòÖ",
      });

    conversationHistory.push({ role: "user", content: userMsg });
    if (conversationHistory.length > 10) conversationHistory.shift();

    if (!aiEnabled) {
      // Provide a richer local fallback using DB results so chat is still useful
      const tutorsFromDb = await fetchRelevantTutorsFromDB(userMsg, 5);
      const items = await sanitizeTutors(tutorsFromDb);
      const localText = localGenerateReply(userMsg, items);
      conversationHistory.push({ role: "assistant", content: localText });
      return res.json({
        success: true,
        message: localText,
        items,
        fallback: true,
      });
    }

    try {
      // Fetch relevant tutors from DB and provide as context
      const tutorsFromDb = await fetchRelevantTutorsFromDB(userMsg, 5);
      const items = await sanitizeTutors(tutorsFromDb);
      const tutorContext = `Danh s√°ch gia s∆∞ ph√π h·ª£p (t·ªëi ƒëa 5): ${JSON.stringify(
        items,
        null,
        2
      )}`;

      // prefer request-scoped client (set by server.js) to support Gemini/Google wrapper
      const client =
        (req && req.app && req.app.locals && req.app.locals.openai) || openai;
      if (!client) {
        throw new Error(
          "AI client not initialized on server. Check GEMINI_KEY/GOOGLE_API_KEY and server startup logs."
        );
      }
      const modelToUse = resolveModelForClient(client, "gpt-3.5-turbo");
      const completion = await client.chat.completions.create({
        model: modelToUse,
        messages: [
          { role: "system", content: systemPrompt },
          ...conversationHistory,
          {
            role: "system",
            content:
              "S·ª≠ d·ª•ng th√¥ng tin gia s∆∞ sau ƒë·ªÉ tr·∫£ l·ªùi n·∫øu c·∫ßn: " +
              tutorContext,
          },
        ],
        temperature: 0.7,
        max_tokens: 500,
      });

      const reply =
        completion?.choices?.[0]?.message?.content ||
        "Xin l·ªói, t√¥i ch∆∞a nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI.";
      if (completion?.raw)
        console.debug("[AI] completion.raw:", completion.raw);
      conversationHistory.push({ role: "assistant", content: reply });
      return res.json({ success: true, message: reply, items });
    } catch (aiErr) {
      console.error("[AI] chatCompletion error object:", aiErr);
      logOpenAIError(aiErr);
      // Extract useful error detail for frontend debugging (non-sensitive)
      let errorDetail = { message: aiErr?.message || "OpenAI error" };
      try {
        if (aiErr?.response) {
          errorDetail.status = aiErr.response.status;
          errorDetail.body = aiErr.response.data || aiErr.response;
        }
      } catch (e) {
        // ignore
      }

      // Provide DB-based fallback suggestions to keep chat useful (using local generator)
      const tutorsFromDb = await fetchRelevantTutorsFromDB(userMsg, 5);
      const items = await sanitizeTutors(tutorsFromDb);
      const localText = localGenerateReply(userMsg, items);
      conversationHistory.push({ role: "assistant", content: localText });
      // Return fallback as HTTP 200 so frontends using axios don't throw on non-2xx
      return res.json({
        success: true,
        message: localText,
        items,
        fallback: true,
        errorDetail,
      });
    }
  } catch (err) {
    console.error("[AI] chatCompletion error:", err);
    return res.json({
      success: false,
      message: "L·ªói khi x·ª≠ l√Ω chat",
      fallback: true,
    });
  }
};

// ===== Tutor search with AI + MongoDB =====
const searchTutorsWithAI = async (req, res) => {
  try {
    const { query } = req.body;
    if (!query || !String(query).trim())
      return res
        .status(400)
        .json({ success: false, message: "B·∫°n ch∆∞a nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm." });

    // ===== Ph√¢n t√≠ch query =====
    let parsedAnalysis = {
      subjects: [],
      levels: [],
      mode: null,
      location: null,
      other: query,
    };

    if (aiEnabled) {
      const analysisPrompt = `Ph√¢n t√≠ch y√™u c·∫ßu t√¨m gia s∆∞ sau v√† tr√≠ch xu·∫•t c√°c th√¥ng tin quan tr·ªçng:\n"${query}"\nTr·∫£ v·ªÅ JSON v·ªõi fields: subjects, levels, mode, location, other.`;
      try {
        const client =
          (req && req.app && req.app.locals && req.app.locals.openai) || openai;
        const analysis = await client.chat.completions.create({
          model: resolveModelForClient(client, "gpt-3.5-turbo"),
          messages: [
            {
              role: "system",
              content: "B·∫°n l√† m·ªôt tr·ª£ l√Ω tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c.",
            },
            { role: "user", content: analysisPrompt },
          ],
          temperature: 0,
        });
        if (analysis?.raw) console.debug("[AI] analysis.raw:", analysis.raw);
        try {
          const rawAnalysis = analysis?.choices?.[0]?.message?.content || "";
          if (rawAnalysis && rawAnalysis.trim().startsWith("{")) {
            parsedAnalysis = JSON.parse(rawAnalysis);
          } else {
            console.warn(
              "[AI] Analysis did not return JSON. Raw:",
              rawAnalysis
            );
            // keep parsedAnalysis as defaults (do not disable AI entirely)
          }
        } catch (e) {
          console.error("[AI] Failed to parse analysis JSON:", e.message);
          // keep aiEnabled as-is; we'll fallback to simple parsing later
        }
      } catch (aiErr) {
        console.error("[AI] Analysis error:", aiErr);
        aiEnabled = false;
      }
    }

    if (!aiEnabled) {
      const q = String(query).toLowerCase();
      if (q.includes("to√°n")) parsedAnalysis.subjects.push("To√°n");
      if (q.includes("anh")) parsedAnalysis.subjects.push("Ti·∫øng Anh");
      if (q.includes("h√† n·ªôi")) parsedAnalysis.location = "H√† N·ªôi";
      if (q.includes("tp.hcm") || q.includes("h·ªì ch√≠ minh"))
        parsedAnalysis.location = "TP.HCM";
    }

    // ===== MongoDB query =====
    const queryConditions = [];
    if (parsedAnalysis.subjects?.length)
      queryConditions.push({ subjects: { $in: parsedAnalysis.subjects } });
    if (parsedAnalysis.levels?.length)
      queryConditions.push({ levels: { $in: parsedAnalysis.levels } });
    if (parsedAnalysis.mode)
      queryConditions.push({ preferredMode: parsedAnalysis.mode });
    if (parsedAnalysis.location)
      queryConditions.push({ "locations.city": parsedAnalysis.location });

    const mongoQuery = queryConditions.length ? { $and: queryConditions } : {};
    const tutors = await TutorProfile.find(mongoQuery)
      .populate("user", "full_name avatar")
      .limit(5)
      .lean();

    const items = await sanitizeTutors(tutors);

    // ===== AI t·∫°o ph·∫£n h·ªìi (n·∫øu c√≥) =====
    if (aiEnabled && items.length > 0) {
      try {
        const tutorContext = `Danh s√°ch gia s∆∞ ph√π h·ª£p (t·ªëi ƒëa 5): ${JSON.stringify(
          items,
          null,
          2
        )}`;
        const responsePrompt = `D·ª±a tr√™n y√™u c·∫ßu: "${query}"\n${tutorContext}\nT·∫°o ph·∫£n h·ªìi th√¢n thi·ªán gi·ªõi thi·ªáu c√°c l·ª±a ch·ªçn, n√™u ng·∫Øn g·ªçn ƒëi·ªÉm m·∫°nh/ƒëi·ªÉm y·∫øu v√† h∆∞·ªõng d·∫´n c√°ch ƒë·∫∑t l·ªãch d·ª±a tr√™n th√¥ng tin.`;
        const client2 =
          (req && req.app && req.app.locals && req.app.locals.openai) || openai;
        if (!client2) {
          throw new Error(
            "AI client not initialized on server. Check GEMINI_KEY/GOOGLE_API_KEY and server startup logs."
          );
        }
        const aiResponse = await client2.chat.completions.create({
          model: resolveModelForClient(client2, "gpt-3.5-turbo"),
          messages: [
            { role: "system", content: systemPrompt },
            { role: "user", content: responsePrompt },
          ],
          temperature: 0.7,
        });
        if (aiResponse?.raw)
          console.debug("[AI] aiResponse.raw:", aiResponse.raw);
        const aiMessage = aiResponse?.choices?.[0]?.message?.content || null;
        return res.json({
          success: true,
          tutors,
          items,
          message: aiMessage,
          analysis: parsedAnalysis,
        });
      } catch (err) {
        console.error("[AI] Response error:", err);
        // attach extra error info for debugging
        const errDetail = { message: err?.message || "AI error" };
        if (err?.response) errDetail.response = err.response;
        // fall back to DB-based response using local generator
        aiEnabled = false;
        console.error(
          "[AI] Falling back to DB results. Error detail:",
          errDetail
        );
        const localText = localGenerateReply(query, items);
        return res.json({
          success: true,
          tutors,
          items,
          message: localText,
          analysis: parsedAnalysis,
          fallback: true,
          errorDetail: errDetail,
        });
      }
    }

    // ===== Fallback message n·∫øu kh√¥ng t√¨m th·∫•y ho·∫∑c AI l·ªói =====
    const summary = tutors.length
      ? `T√¨m ƒë∆∞·ª£c ${tutors.length} gia s∆∞ ph√π h·ª£p.`
      : `Xin l·ªói, m√¨nh ch∆∞a t√¨m th·∫•y gia s∆∞ ph√π h·ª£p v·ªõi "${query}". B·∫°n th·ª≠ t·ª´ kh√≥a kh√°c nh√© üòä`;

    return res.json({
      success: true,
      tutors,
      items,
      message: summary,
      analysis: parsedAnalysis,
      fallback: true,
    });
  } catch (error) {
    console.error("[AI] searchTutorsWithAI error:", error);
    return res.json({
      success: true,
      tutors: [],
      message: "Hi·ªán t·∫°i kh√¥ng th·ªÉ t√¨m gia s∆∞, b·∫°n th·ª≠ l·∫°i sau.",
      analysis: {},
      fallback: true,
    });
  }
};

module.exports = {
  chatCompletion,
  searchTutorsWithAI,
};
